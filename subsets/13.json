[
    {
    "front": "joint probability mass function<br>(pair of random variables)",
    "back": "For a pair of random variables $(X,Y)$, the joint <i>probability mass function</i> (PMF) defines the probability that $(X,Y)$ takes on each value $(x,y) \\in \\mathbb{R}^2$, \\begin{align*} P_{X,Y} (x,y) &= P\\left[  \\left\\{ s \\left| X(s) = x, Y(s) =y \\right. \\right\\} \\right] \\\\ &= P \\left[  X=x, Y=y \\right]. \\end{align*}"
},
    {
    "front": "joint cumulative distribution function<br>(pair of random variables)",
    "back": "For a pair of random variables $(X,Y)$, the joint <i>cumulative distribution function</i> (CDF) is \\begin{align*} F_{XY} (x,y) &= P\\left[  \\left\\{ s \\left| X(s) \\le x, Y(s) \\le y \\right. \\right\\} \\right] \\\\ &= P \\left[  X \\le x, Y \\le y \\right]. \\end{align*}"
},
    {
    "front": "joint probability density function<br>(pair of random variables)",
    "back": "For a pair of random variables $(X,Y)$, the joint <i>probability density function</i> (pdf) is \\begin{align*} f_{XY} (x,y) &= \\frac{\\partial^2}{\\partial x ~\\partial y} F_{XY} (x,y). \\end{align*}"
},
    {
    "front": "marginal probability density function<br>(pair of random variables)",
    "back": "For a pair of random variables $(X,Y)$ with joint pdf $f_{XY} (x,y)$, the <i>marginal density</i> functions of $X$ and $Y$ are the individual pdfs $f_X(x)$ and $f_Y(y)$. They can be calculated from the joint pdf as \\begin{align*} f_X(x) &=  \\int_{-\\infty}^{\\infty} f_{XY} (x,y) dy , \\mbox{ and}\\\\ f_Y(y) &=  \\int_{-\\infty}^{\\infty} f_{XY} (x,y) dx . \\end{align*}"
},
    {
    "front": "contour of equal probability density<br>(pair of random variables)",
    "back": "Given some value $a$ that the joint density takes on, the corresponding <i>contour of equal probability density</i> $\\mathcal{C}_a$ is defined as the set of $(x,y)$ values that achieve that density. I.e., if the joint density is $f_{XY}(x,y)$, then \\begin{align*} \\mathcal{C}_a = \\left\\{ (x,y) ~\\vert~ f(x,y) =c \\right\\}. \\end{align*}"
},
    {
    "front": "random vector",
    "back": "A random vector $\\mathbf{X}= \\left[X_0, X_1, \\ldots, X_{n-1}\\right]^T$ is an ordered collection of random variables. Formally, a random vector is defined on a  probability space $(S, \\mathcal{F}, P)$ and is a function $\\mathbf{X}(s)$ that maps from the sample space to $\\mathbf{R}^n$."
},
    {
    "front": "mean vector",
    "back": "For a random vector $\\mathbf{X}= \\left[X_0, X_1, \\ldots, X_{n-1}\\right]^T$, the <i>mean vector</i> $\\boldsymbol{\\mu}$ is the $n$-dimensional vector whose $i$th entry is the mean of $X_i$; i.e. $\\boldsymbol{\\mu}_i = E\\left[X_i\\right]$."
},
   {
    "front": "covariance matrix",
    "back": "For a random vector $\\mathbf{X}= \\left[X_0, X_1, \\ldots, X_{n-1}\\right]$, the <i>covariance matrix</i> $K$ is the $n \\times n$ matrix whose $i,j$th entry is \\begin{align*} \\mathbf{K}_{ij} &= \\operatorname{Cov}(X_i, X_j) \\\\ &= E \\left[ \\left(X_i - \\mu_i \\right) \\left(X_j - \\mu_j\\right) \\right]. \\end{align*}"
},
    {
    "front": "correlation coefficient<br>(random variables)",
    "back": "For jointly distributed random variables $X$ and $Y$, the <i>correlation coefficient</i> is \\begin{align*} \\rho &= \\frac{\\operatorname{Cov}(X, Y)}{\\sigma_X \\sigma_y}, \\end{align*} where $\\sigma_{X}^{2}$ and $\\sigma_{Y}^{2}$ are the variances of $X$ and $Y$."
},
    {
    "front": "uncorrelated",
    "back": "Jointly distributed random variables $X$ and $Y$ are uncorrelated if and only if $\\rho=0$, or equivalently $\\operatorname{Cov}(X,Y) =0$."
},
    {
    "front": "correlation matrix",
    "back": "For a random vector $\\mathbf{X}= \\left[X_0, X_1, \\ldots, X_{n-1}\\right]$, the <i>correlation matrix</i> $\\mathbf{R}$ is the $n \\times n$ matrix whose $i,j$th entry is \\begin{align*} \\mathbf{R}_{ij} &= \\rho_{i,j} \\\\ &= \\frac{\\operatorname{Cov}\\left(X_i, X_j\\right)}{\\sigma_i \\sigma_j}. \\end{align*}"
},
    {
    "front": "iid",
    "back": "Random variables are <i>independent and identically distributed (iid)</i> if they have the same distributions and are independent."
},
    {
    "front": "broadcasting",
    "back": "In NumPy, a smaller array is <i>broadcast</i> across a larger array in such a way that the dimensions of the two arrays will match. "
},
    {
    "front": "standardization",
    "back": "The process by which numerical data is transformed such that each feature has mean zero and variance one."
},
    {
    "front": "eigenvector",
    "back": "Given a $n \\times n$ matrix $\\mathbf{M}$,  a non-zero vector $\\mathbf{v}$ is an <i>eigenvector</i> of $\\mathbf{M}$ if \\begin{equation*} \\mathbf{M} \\mathbf{v} = \\lambda \\mathbf{v} \\end{equation*} for some constant $\\lambda$."
},
    {
    "front": "eigenvalue",
    "back": "Given a $n \\times n$ matrix $\\mathbf{M}$, if  $\\mathbf{v}$ is an eigenvector of $\\mathbf{M}$, then the corresponding <i>eigenvalue</i> is the constant $\\lambda$ such that \\begin{equation*} \\mathbf{M} \\mathbf{v} = \\lambda \\mathbf{v}. \\end{equation*}"
},
    {
    "front": "characteristic equation",
    "back": "Given a $n \\times n$ matrix $\\mathbf{M}$, the <i>characteristic equation</i> is \\begin{equation*} \\det \\left(\\mathbf{M} - \\lambda \\mathbf{I} \\right) =0, \\end{equation*} which can be used to solve for the eigenvalues ($\\lambda$)."
},
    {
    "front": "modal matrix",
    "back": "For a square matrix $\\mathbf{M}$, the matrix whose columns are the eigenvectors of $\\mathbf{M}$. "
},
    {
    "front": "eigendecomposition",
    "back": " Suppose $\\mathbf{M}$ is a real $n \\times n$ matrix with modal matrix $\\mathbf{V}$ and eigenvalue matrix $\\boldsymbol{\\Lambda}$. If $\\mathbf{V}$ has full rank, then the <i>eigendecomposition</i> (diagonalization) of $\\mathbf{M}$ is \\begin{equation*} \\mathbf{M}= \\mathbf{V} \\boldsymbol{\\Lambda}\\mathbf{V}^{-1} . \\end{equation*}    "
},
    {
    "front": "relating determinant and eigenvalues",
    "back": " If $\\mathbf{M}$ has an eigendecomposition with eigenvalues $\\lambda_i$ then \\begin{equation*} \\det \\mathbf{M} = \\prod_{i} \\lambda_{i} . \\end{equation*}"
},
    {
    "front": "dimensionality reduction",
    "back": "The process of going from a high-dimensionality data set to a lower-dimensionality data set, while preserving as much important information as possible."
},
    {
    "front": "Karhunen-Lo√®ve Transform<br>(KLT)",
    "back": "Suppose  $\\mathbf{X}$ has non-singular covariance matrix $\\mathbf{K}_X$, with eigendecomposition $\\mathbf{K}_X = \\mathbf{U} \\boldsymbol{\\Lambda}\\mathbf{U}^T$. Then $\\mathbf{Y} = \\mathbf{U}^T \\mathbf{X}$ consists of uncorrelated random variables and $\\mathbf{K}_Y = \\boldsymbol{\\Lambda}.$"
},
    {
    "front": "principal components analysis<br>(PCA)",
    "back": "Given a multi-dimensional data set, <I>PCA</I> is the process by which the data is decorrelated using the modal matrix of the <B>sample covariance matrix</B>. "
},
    {
    "front": "scree plot",
    "back": "In PCA, a <I>scree plot</I> is a line plot that illustrates the eigenvalues of the covariance matrix of a multidimensional dataset."
},
    {
    "front": "explained variance",
    "back": "In dimensionality reduction, the <I>explained variance</I> is the ratio of the total variance after dimensionality reduction to the total variance of the original data set."
},
    {
    "front": "test-train split",
    "back": "The data is randomly partitioned into a training set and a testing set. The training set is used to train an algorithm or develop a model and usually uses the majority of the data; typical values are 75% of the total data. The testing set is used to make sure that the algorithm or model is not overfit; it consists of all the data not in the training set."
}
]
